{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Next-word Predictor",
      "provenance": [],
      "authorship_tag": "ABX9TyMUZqqy986NF5MAgoN/mCi0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vtkachuk4/next_word_predictor/blob/master/Next_word_Predictor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M73FtkKi8q6I",
        "colab_type": "text"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IHCeLQ4-8ths",
        "colab_type": "text"
      },
      "source": [
        "This Notebook performs next word prediction based on Shakespeare text using a pre-trained RNN model that uses a Universal Sentence Encoder for word embeddings.\n",
        "\n",
        "The model was trained on Shakespeare text so the next word predictions are in the style of Shakespeare :)\n",
        "\n",
        "The accompanying model and training code is in the Github repository: https://github.com/vtkachuk4/next_word_predictor.git \n",
        "In Notebook: word_based_word_predictor_model\n",
        "\n",
        "After the model was trained it was saved, downloaded and then uploaded to the corresponding Github repository for later use.\n",
        "\n",
        "The source code for this model is largly based on the sample code provided here: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb#scrollTo=WvuwZBX5Ogfd\n",
        "\n",
        "The char based word predictor was changed to a word based word predictor and trained using a pre-trained a Universal Sentence Encoder: https://tfhub.dev/google/universal-sentence-encoder/4 for word embeddings.\n",
        "\n",
        "All sources are cited as comments above any other code used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6PesGiD33f2",
        "colab_type": "text"
      },
      "source": [
        "# Setup\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7SvBRbq39Ej",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aAB7GlP4f3C",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "92c95ad3-6874-4d11-bd7b-414f8d0bdd02"
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6kCwEeP4joN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEuLsg8v4nkp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# source: https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
        "# used for cleaning up the Shakespeare text\n",
        "import string\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # replace '--' with a space ' '\n",
        "    doc = doc.replace('--', ' ')\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # make lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKglG_oN4rhN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e1fa5eee-439f-45ef-8066-97a557b41e40"
      },
      "source": [
        "# source: https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
        "# Double check the text is actually clean\n",
        "tokens = clean_doc(text)\n",
        "unique_words = sorted(set(tokens))\n",
        "print(tokens[:20])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['first', 'citizen', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', 'all', 'speak', 'speak', 'first', 'citizen', 'you', 'are', 'all', 'resolved', 'rather']\n",
            "Total Tokens: 202820\n",
            "Unique Tokens: 12669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQ8xYTrk4vuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is just done for allow easy integration with the previous sample code\n",
        "vocab = unique_words\n",
        "text = tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SOJ1l-yD4y4o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique words to indices\n",
        "word2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2word = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([word2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i2jQlb4m47Nb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "b7eb7e63-7bc9-4aa6-f56c-352e622617c9"
      },
      "source": [
        "# load the model saved in the Github repo\n",
        "!git clone https://github.com/vtkachuk4/next_word_predictor.git\n",
        "\n",
        "next_word_predictor_model = tf.keras.models.load_model('next_word_predictor/models/model_10_epochs')\n",
        "\n",
        "# Check its architecture\n",
        "next_word_predictor_model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'next_word_predictor' already exists and is not an empty directory.\n",
            "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_3 (Embedding)      (1, None, 512)            6486528   \n",
            "_________________________________________________________________\n",
            "gru_3 (GRU)                  (1, None, 1024)           4724736   \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (1, None, 12669)          12985725  \n",
            "=================================================================\n",
            "Total params: 24,196,989\n",
            "Trainable params: 24,196,989\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJKX3sxU7Wve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_tokens):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of words to generate\n",
        "  num_generate = 1\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [word2idx[s] for s in start_tokens]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2word[predicted_id])\n",
        "\n",
        "  return (' '.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5DX2fAt6Ix9",
        "colab_type": "text"
      },
      "source": [
        "# Next-word Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad0_tp3R8Wb9",
        "colab_type": "text"
      },
      "source": [
        "Enter any start sentence (start_words) you would like that is at least 1 word in length"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "86RIX8pZ6MD_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start_words = ['romeo', 'oh', 'art', 'thou']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNRPt7aT8iyA",
        "colab_type": "text"
      },
      "source": [
        "The following will print the predicted word and the sentence including the predicted word"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5m4DwMM7Oj-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "60af3ea3-7a22-4584-c809-cb931927b20e"
      },
      "source": [
        "next_word_prediction = generate_text(next_word_predictor_model, start_tokens=start_words)\n",
        "print(f'Next-word prediction: {next_word_prediction}')\n",
        "print (f'Full sentence with predicted word: {\" \".join(start_words) + \" \" + next_word_prediction}')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Next-word prediction: time\n",
            "Full sentence with predicted word: romeo oh art thou time\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}