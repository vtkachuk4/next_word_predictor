{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word_based_word_predictor_model",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "mount_file_id": "1GJ0yefVYOe1pBZUlQEVwk-Sl-e3KIetZ",
      "authorship_tag": "ABX9TyPob2i6raEyjl7aEajirIga",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vtkachuk4/next_word_predictor/blob/master/word_based_word_predictor_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97xi59Rl9VZB",
        "colab_type": "text"
      },
      "source": [
        "# Overview"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk_Qc4Qv9ZLP",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Next word predictor model is created and trained in this notebook.\n",
        "\n",
        "The model was trained on Shakespeare text so the next word predictions are in the style of Shakespeare :)\n",
        "\n",
        "The accompanying next-word predictor is in the Github repository: https://github.com/vtkachuk4/next_word_predictor.git In Notebook: Next-word Predictor\n",
        "\n",
        "After the model was trained it was saved, downloaded and then uploaded to the corresponding Github repository for later use.\n",
        "\n",
        "The source code for this model is largly based on the sample code provided here: https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/text_generation.ipynb#scrollTo=WvuwZBX5Ogfd\n",
        "\n",
        "The char based word predictor was changed to a word based word predictor and trained using a pre-trained a Universal Sentence Encoder: https://tfhub.dev/google/universal-sentence-encoder/4 for word embeddings.\n",
        "\n",
        "All sources are cited as comments above any other code used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XV2VG4o9gDm",
        "colab_type": "text"
      },
      "source": [
        "# Model Creation and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgLiLfIvEkUr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5QYG5-HE4Zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C7epr1eFJKz",
        "colab_type": "code",
        "outputId": "38125b26-1cfe-4f90-aed4-228e95a74ca0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of words in it\n",
        "print ('Length of text: {} words'.format(len(text)))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Length of text: 1115394 words\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0tIDOGxUC_I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# source: https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
        "# used for cleaning up the Shakespeare text\n",
        "import string\n",
        "\n",
        "# turn a doc into clean tokens\n",
        "def clean_doc(doc):\n",
        "    # replace '--' with a space ' '\n",
        "    doc = doc.replace('--', ' ')\n",
        "    # split into tokens by white space\n",
        "    tokens = doc.split()\n",
        "    # remove punctuation from each token\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    tokens = [w.translate(table) for w in tokens]\n",
        "    # remove remaining tokens that are not alphabetic\n",
        "    tokens = [word for word in tokens if word.isalpha()]\n",
        "    # make lower case\n",
        "    tokens = [word.lower() for word in tokens]\n",
        "    return tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RyGgwJiUDXG",
        "colab_type": "code",
        "outputId": "717d1efa-89ae-4d6c-a2bb-82c0870fb2c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# source: https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
        "# Double check the text is actually clean\n",
        "tokens = clean_doc(text)\n",
        "unique_words = sorted(set(tokens))\n",
        "print(tokens[:20])\n",
        "print('Total Tokens: %d' % len(tokens))\n",
        "print('Unique Tokens: %d' % len(set(tokens)))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['first', 'citizen', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', 'all', 'speak', 'speak', 'first', 'citizen', 'you', 'are', 'all', 'resolved', 'rather']\n",
            "Total Tokens: 202820\n",
            "Unique Tokens: 12669\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKwLnWS3Vc__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# This is just done for allow easy integration with the previous sample code\n",
        "vocab = unique_words\n",
        "text = tokens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_uMMXUNFouz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating a mapping from unique words to indices\n",
        "word2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2word = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([word2idx[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeeYJNtfF9oO",
        "colab_type": "code",
        "outputId": "77f76459-a029-4953-d37f-d0ab1854d2cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# print the first 20 words and corresponding int representations\n",
        "print('{')\n",
        "for word,_ in zip(word2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(word), word2idx[word]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  'a' :   0,\n",
            "  'abandond':   1,\n",
            "  'abase':   2,\n",
            "  'abate':   3,\n",
            "  'abated':   4,\n",
            "  'abbey':   5,\n",
            "  'abbot':   6,\n",
            "  'abed':   7,\n",
            "  'abels':   8,\n",
            "  'abet':   9,\n",
            "  'abhor':  10,\n",
            "  'abhorrd':  11,\n",
            "  'abhorred':  12,\n",
            "  'abhorring':  13,\n",
            "  'abhors':  14,\n",
            "  'abhorson':  15,\n",
            "  'abide':  16,\n",
            "  'abides':  17,\n",
            "  'abilities':  18,\n",
            "  'ability':  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFev1PK5GE4B",
        "colab_type": "code",
        "outputId": "625a9869-f7f6-42a6-9bdf-96df24ac827c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "# Show how the first 13 words from the text are mapped to integers\n",
        "print ('{} ---- words mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['first', 'citizen', 'before', 'we', 'proceed', 'any', 'further', 'hear', 'me', 'speak', 'all', 'speak', 'speak'] ---- words mapped to int ---- > [ 4143  1933   924 12174  8393   450  4536  5129  6744 10182   302 10182\n",
            " 10182]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mko0MGq-G7vp",
        "colab_type": "code",
        "outputId": "49fd0cce-8980-4fe3-fb11-34638da5072f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# The maximum length sentence we want for a single input in words\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "word_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in word_dataset.take(5):\n",
        "  print(idx2word[i.numpy()])"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "first\n",
            "citizen\n",
            "before\n",
            "we\n",
            "proceed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZP-lEjPH-V5",
        "colab_type": "code",
        "outputId": "3e248c2e-b2ea-4199-c7b6-9c8dd4e24c8d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# print first five sequences\n",
        "sequences = word_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(' '.join(idx2word[item.numpy()])))"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'first citizen before we proceed any further hear me speak all speak speak first citizen you are all resolved rather to die than to famish all resolved resolved first citizen first you know caius marcius is chief enemy to the people all we knowt we knowt first citizen let us kill him and well have corn at our own price ist a verdict all no more talking ont let it be done away away second citizen one word good citizens first citizen we are accounted poor citizens the patricians good what authority surfeits on would relieve us if they would yield'\n",
            "'us but the superfluity while it were wholesome we might guess they relieved us humanely but they think we are too dear the leanness that afflicts us the object of our misery is as an inventory to particularise their abundance our sufferance is a gain to them let us revenge this with our pikes ere we become rakes for the gods know i speak this in hunger for bread not in thirst for revenge second citizen would you proceed especially against caius marcius all against him first hes a very dog to the commonalty second citizen consider you what services he'\n",
            "'has done for his country first citizen very well and could be content to give him good report fort but that he pays himself with being proud second citizen nay but speak not maliciously first citizen i say unto you what he hath done famously he did it to that end though softconscienced men can be content to say it was for his country he did it to please his mother and to be partly proud which he is even till the altitude of his virtue second citizen what he cannot help in his nature you account a vice in him'\n",
            "'you must in no way say he is covetous first citizen if i must not i need not be barren of accusations he hath faults with surplus to tire in repetition what shouts are these the other side o the city is risen why stay we prating here to the capitol all come come first citizen soft who comes here second citizen worthy menenius agrippa one that hath always loved the people first citizen hes one honest enough would all the rest were so menenius what works my countrymen in hand where go you with bats and clubs the matter speak'\n",
            "'i pray you first citizen our business is not unknown to the senate they have had inkling this fortnight what we intend to do which now well show em in deeds they say poor suitors have strong breaths they shall know we have strong arms too menenius why masters my good friends mine honest neighbours will you undo yourselves first citizen we cannot sir we are undone already menenius i tell you friends most charitable care have the patricians of you for your wants your suffering in this dearth you may as well strike at the heaven with your staves as'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENVprkqiIKMc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# create outpur sequence to be offset from input by 1 word\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q11KX_rCIeJC",
        "colab_type": "code",
        "outputId": "7be4df85-5ea9-4e6f-96f6-2909e80e5534",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(1):\n",
        "  print ('Input data: ', repr(' '.join(idx2word[input_example.numpy()])))\n",
        "  print ('Target data:', repr(' '.join(idx2word[target_example.numpy()])))"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input data:  'first citizen before we proceed any further hear me speak all speak speak first citizen you are all resolved rather to die than to famish all resolved resolved first citizen first you know caius marcius is chief enemy to the people all we knowt we knowt first citizen let us kill him and well have corn at our own price ist a verdict all no more talking ont let it be done away away second citizen one word good citizens first citizen we are accounted poor citizens the patricians good what authority surfeits on would relieve us if they would'\n",
            "Target data: 'citizen before we proceed any further hear me speak all speak speak first citizen you are all resolved rather to die than to famish all resolved resolved first citizen first you know caius marcius is chief enemy to the people all we knowt we knowt first citizen let us kill him and well have corn at our own price ist a verdict all no more talking ont let it be done away away second citizen one word good citizens first citizen we are accounted poor citizens the patricians good what authority surfeits on would relieve us if they would yield'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iEGNAwyMI24a",
        "colab_type": "code",
        "outputId": "8cd6e2cd-dae3-4573-9a73-4014ecb0c92d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"Step {:4d}\".format(i))\n",
        "    print(\"  input: {} ({:s})\".format(input_idx, repr(idx2word[input_idx])))\n",
        "    print(\"  expected output: {} ({:s})\".format(target_idx, repr(idx2word[target_idx])))"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Step    0\n",
            "  input: 4143 ('first')\n",
            "  expected output: 1933 ('citizen')\n",
            "Step    1\n",
            "  input: 1933 ('citizen')\n",
            "  expected output: 924 ('before')\n",
            "Step    2\n",
            "  input: 924 ('before')\n",
            "  expected output: 12174 ('we')\n",
            "Step    3\n",
            "  input: 12174 ('we')\n",
            "  expected output: 8393 ('proceed')\n",
            "Step    4\n",
            "  input: 8393 ('proceed')\n",
            "  expected output: 450 ('any')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW9hXQLsJRT7",
        "colab_type": "code",
        "outputId": "a8d3579f-bfdb-44c9-92bc-4748a9f4ef5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iNEgvW4jKE1r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in words\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension, based on universal sentence encoder output\n",
        "embedding_dim = 512\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PbmF5hwtgM3p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# source: https://tfhub.dev/google/universal-sentence-encoder/4\n",
        "import tensorflow_hub as hub\n",
        "embed = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "# source: https://medium.com/@pi19404/using-pre-trained-word-vector-embeddings-for-sequence-classification-using-lstm-277dee188348\n",
        "# create an embedding_matrix that maps word idx representations to their \n",
        "# corresponding word embeddings\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word in vocab:\n",
        "    embedding = embed([word]).numpy()[0]\n",
        "    embedding_matrix[word2idx[word]] = embedding\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-aHtU-dKfJh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# def a build_model function that uses pre-trained Universal Sentence Encoder \n",
        "# for word Embeddings\n",
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              weights = [embedding_matrix],\n",
        "                              batch_input_shape=[batch_size, None],\n",
        "                              trainable=False),\n",
        "    tf.keras.layers.GRU(rnn_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfZE4qRhLrjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSHn8y-vMp9m",
        "colab_type": "code",
        "outputId": "7dc6be3f-dfca-47cc-d73d-016f40442723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 12669) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CsKal8TNAm2",
        "colab_type": "code",
        "outputId": "ef5529bf-6b9a-41d1-b5e6-cf26ff40407e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_4 (Embedding)      (64, None, 512)           6486528   \n",
            "_________________________________________________________________\n",
            "gru_4 (GRU)                  (64, None, 1024)          4724736   \n",
            "_________________________________________________________________\n",
            "dense_4 (Dense)              (64, None, 12669)         12985725  \n",
            "=================================================================\n",
            "Total params: 24,196,989\n",
            "Trainable params: 17,710,461\n",
            "Non-trainable params: 6,486,528\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUJ6L0qjNhd7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Agqwx_BGNtvu",
        "colab_type": "code",
        "outputId": "0fc7639e-c795-448c-ce3a-c84ea8ec15e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 8360, 11220,  9968,  2005,  4103, 10214, 11890,  7272,  3780,\n",
              "       11352, 11222,  5920,  4668,  7819,   343,  9084,  2534, 11293,\n",
              "        1012,  2978,  7378,  2768,  9326,  5201, 12011,  5824,  8538,\n",
              "        9397, 11083,  2306,  3804,  6950,  7286, 10304, 10581,  2367,\n",
              "        9146,  1131,  5604,  6454, 10701,  7720,  6276,  3688,   911,\n",
              "        3300,  8955,  7732, 11823,  6279,  1493,  4226,  5804,  8297,\n",
              "        5991,  5114,  1036,  3894,  4656,  8545, 11948,   580, 12597,\n",
              "       10467,  3863,  9072,  4738, 12421,  9068, 10134, 12567, 11085,\n",
              "       10327,  8149,  9686,  5852,  3684,  4195, 10836,  8979,  7448,\n",
              "        5740, 10692, 11731,  6469,  3143,  2181,  1490,  1950, 12017,\n",
              "        9606,   340, 11774,  1021,  8782,  4923,  4128,  2560,  5566,\n",
              "       11144])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1-vQvRxN5HR",
        "colab_type": "code",
        "outputId": "456594ea-f7a4-452e-ecb9-e9fe73733918",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "print(\"Input: \\n\", repr(\" \".join(idx2word[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next word Predictions: \\n\", repr(\" \".join(idx2word[sampled_indices ])))"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: \n",
            " 'it was senseless twas nothing to geld a codpiece of a purse i could have filed keys off that hung in chains no hearing no feeling but my sirs song and admiring the nothing of it so that in this time of lethargy i picked and cut most of their festival purses and had not the old man come in with a whoobub against his daughter and the kings son and scared my choughs from the chaff i had not left a purse alive in the whole army camillo nay but my letters by this means being there so soon'\n",
            "\n",
            "Next word Predictions: \n",
            " 'priesthood tongue sleepy clouded filed spheres vassals new excepting tribunes tongues jouncing gibingly patron alter revive crave traind bennet determinate nourish dearth saint helmet visitors invisible psalms scandalous threatened contain exhibit misery newness stabs subject cony ripening bitterness imprisont loose suppresseth palsy letters entrails bedward doubleness repented paper urging level bull flock interred preparation kernels headlong beshrew fairly get pudding vicar ashamed wrongd stories eyne revenged goads winking revel sounded wrappd threatens stampd pointed she issues entirely flatteringsweet taken reprobate ocean inshelld supportance unseasonable lot disinherited concernings built claps vizardlike serviceable alps untouchd bereft rebuke grudges fingers crew impartial thwarted'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLWNkoU1OIbG",
        "colab_type": "code",
        "outputId": "fd43a386-99a4-45bb-d2ac-3a919d56a731",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (64, 100, 12669)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       9.44701\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imUcepDZOVl3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVvfQVnoObsY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxbXQPdEOmp3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdWTrrDAOscL",
        "colab_type": "code",
        "outputId": "39425395-2827-436f-d661-4289815ece32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "31/31 [==============================] - 4s 135ms/step - loss: 7.4421\n",
            "Epoch 2/10\n",
            "31/31 [==============================] - 5s 153ms/step - loss: 6.9450\n",
            "Epoch 3/10\n",
            "31/31 [==============================] - 4s 135ms/step - loss: 6.8640\n",
            "Epoch 4/10\n",
            "31/31 [==============================] - 4s 138ms/step - loss: 6.7744\n",
            "Epoch 5/10\n",
            "31/31 [==============================] - 4s 132ms/step - loss: 6.6897\n",
            "Epoch 6/10\n",
            "31/31 [==============================] - 4s 135ms/step - loss: 6.6113\n",
            "Epoch 7/10\n",
            "31/31 [==============================] - 5s 151ms/step - loss: 6.5423\n",
            "Epoch 8/10\n",
            "31/31 [==============================] - 5s 145ms/step - loss: 6.4953\n",
            "Epoch 9/10\n",
            "31/31 [==============================] - 5s 150ms/step - loss: 6.4387\n",
            "Epoch 10/10\n",
            "31/31 [==============================] - 4s 136ms/step - loss: 6.3928\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVBgIMuuPpCW",
        "colab_type": "code",
        "outputId": "0f169bc6-e409-4f42-b7d6-cecbb4aab5d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUkOCAkUPtJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lR8xTRCP60z",
        "colab_type": "code",
        "outputId": "4cb5fde7-ac7a-440d-e189-a4b6909a41a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_5 (Embedding)      (1, None, 512)            6486528   \n",
            "_________________________________________________________________\n",
            "gru_5 (GRU)                  (1, None, 1024)           4724736   \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (1, None, 12669)          12985725  \n",
            "=================================================================\n",
            "Total params: 24,196,989\n",
            "Trainable params: 17,710,461\n",
            "Non-trainable params: 6,486,528\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0_RycJqeQhjp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_tokens):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of words to generate\n",
        "  num_generate = 1\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [word2idx[s] for s in start_tokens]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # We pass the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2word[predicted_id])\n",
        "\n",
        "  return (' '.join(start_tokens) + ' ' + ' '.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UFguLWyeQkJR",
        "colab_type": "code",
        "outputId": "b998a14e-2f7d-4956-9fca-475119d0d1e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(generate_text(model, start_tokens=['romeo', 'oh', 'art', 'thou']))"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "romeo oh art thou said\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "912DZlYGRqaq",
        "colab_type": "code",
        "outputId": "32359c08-ef69-45e1-8e7f-e667ab9de5ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 683
        }
      },
      "source": [
        "# source: https://colab.research.google.com/github/googlecolab/colabtools/blob/master/notebooks/colab-github-demo.ipynb\n",
        "# Save the entire model as a SavedModel.\n",
        "!mkdir -p saved_model\n",
        "model.save('saved_model/model_10_epochs') \n",
        "\n",
        "# my_model directory\n",
        "!ls saved_model\n",
        "\n",
        "# Contains an assets folder, saved_model.pb, and variables folder.\n",
        "!ls saved_model/my_model"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.kernel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-2.bias\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.kernel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.recurrent_kernel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'm' for (root).layer_with_weights-1.cell.bias\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.kernel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-2.bias\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.kernel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.recurrent_kernel\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer's state 'v' for (root).layer_with_weights-1.cell.bias\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/model_10_epochs/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/model_10_epochs/assets\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "model_10_epochs  my_model\n",
            "assets\tsaved_model.pb\tvariables\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m_6iVuANqR8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for mounting drive \n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rsbvAyX80s0-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# to save model in drive so you cna download it\n",
        "#model_save_name = 'model_10_epochs'\n",
        "#path = F\"/content/drive/My Drive/{model_save_name}\" \n",
        "#model.save(path)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}